{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loan_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Married</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Education</th>\n",
       "      <th>Self_Employed</th>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <th>LoanAmount</th>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <th>Credit_History</th>\n",
       "      <th>Property_Area</th>\n",
       "      <th>Loan_Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LP001002</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>5849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LP001003</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>4583</td>\n",
       "      <td>1508.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Rural</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LP001005</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LP001006</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Not Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>2583</td>\n",
       "      <td>2358.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LP001008</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>6000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Loan_ID Gender Married Dependents     Education Self_Employed  \\\n",
       "0  LP001002   Male      No          0      Graduate            No   \n",
       "1  LP001003   Male     Yes          1      Graduate            No   \n",
       "2  LP001005   Male     Yes          0      Graduate           Yes   \n",
       "3  LP001006   Male     Yes          0  Not Graduate            No   \n",
       "4  LP001008   Male      No          0      Graduate            No   \n",
       "\n",
       "   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
       "0             5849                0.0         NaN             360.0   \n",
       "1             4583             1508.0       128.0             360.0   \n",
       "2             3000                0.0        66.0             360.0   \n",
       "3             2583             2358.0       120.0             360.0   \n",
       "4             6000                0.0       141.0             360.0   \n",
       "\n",
       "   Credit_History Property_Area Loan_Status  \n",
       "0             1.0         Urban           Y  \n",
       "1             1.0         Rural           N  \n",
       "2             1.0         Urban           Y  \n",
       "3             1.0         Urban           Y  \n",
       "4             1.0         Urban           Y  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('loan.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 614 entries, 0 to 613\n",
      "Data columns (total 13 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Loan_ID            614 non-null    object \n",
      " 1   Gender             601 non-null    object \n",
      " 2   Married            611 non-null    object \n",
      " 3   Dependents         599 non-null    object \n",
      " 4   Education          614 non-null    object \n",
      " 5   Self_Employed      582 non-null    object \n",
      " 6   ApplicantIncome    614 non-null    int64  \n",
      " 7   CoapplicantIncome  614 non-null    float64\n",
      " 8   LoanAmount         592 non-null    float64\n",
      " 9   Loan_Amount_Term   600 non-null    float64\n",
      " 10  Credit_History     564 non-null    float64\n",
      " 11  Property_Area      614 non-null    object \n",
      " 12  Loan_Status        614 non-null    object \n",
      "dtypes: float64(4), int64(1), object(8)\n",
      "memory usage: 62.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gender               0\n",
       "Married              0\n",
       "Dependents           0\n",
       "Education            0\n",
       "Self_Employed        0\n",
       "ApplicantIncome      0\n",
       "CoapplicantIncome    0\n",
       "LoanAmount           0\n",
       "Loan_Amount_Term     0\n",
       "Credit_History       0\n",
       "Property_Area        0\n",
       "Loan_Status          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task1 deal with NULL rows, you can either choose to drop them or replace them with mean or other value\n",
    "\n",
    "df.drop(\"Loan_ID\", axis=1, inplace=True)\n",
    "np.random.seed(38084)\n",
    "for i in range(df.shape[1]):\n",
    "    empty = df.iloc[:, i].isnull()\n",
    "    df.iloc[empty.values, i] = np.random.choice(\n",
    "        df.iloc[~empty.values, i].values, len(df.iloc[empty.values, i]))\n",
    "\n",
    "# # Checking the Missing Values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Married</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Education</th>\n",
       "      <th>Self_Employed</th>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <th>LoanAmount</th>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <th>Credit_History</th>\n",
       "      <th>Property_Area</th>\n",
       "      <th>Loan_Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5849.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4583.0</td>\n",
       "      <td>1508.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2583.0</td>\n",
       "      <td>2358.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Gender  Married  Dependents  Education  Self_Employed  ApplicantIncome  \\\n",
       "0       1        0         0.0          1              0           5849.0   \n",
       "1       1        1         1.0          1              0           4583.0   \n",
       "2       1        1         0.0          1              1           3000.0   \n",
       "3       1        1         0.0          0              0           2583.0   \n",
       "4       1        0         0.0          1              0           6000.0   \n",
       "\n",
       "   CoapplicantIncome  LoanAmount  Loan_Amount_Term  Credit_History  \\\n",
       "0                0.0        96.0             360.0             1.0   \n",
       "1             1508.0       128.0             360.0             1.0   \n",
       "2                0.0        66.0             360.0             1.0   \n",
       "3             2358.0       120.0             360.0             1.0   \n",
       "4                0.0       141.0             360.0             1.0   \n",
       "\n",
       "   Property_Area  Loan_Status  \n",
       "0            1.0            1  \n",
       "1            0.0            0  \n",
       "2            1.0            1  \n",
       "3            1.0            1  \n",
       "4            1.0            1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task2 deal with categorical features\n",
    "# Tip df.Gender=df.Gender.map({'Male':1,'Female':0})\n",
    "df.Gender = df.Gender.map({'Male': 1, 'Female': 0})\n",
    "df.Married = df.Married.map({'Yes': 1, 'No': 0})\n",
    "df.Education = df.Education.map({'Graduate': 1, 'Not Graduate': 0})\n",
    "df.Self_Employed = df.Self_Employed.map({'Yes': 1, 'No': 0})\n",
    "df.Property_Area = df.Property_Area.map(\n",
    "    {'Urban': 1.0, 'Semiurban': 0.5, 'Rural': 0.0})\n",
    "df.Loan_Status = df.Loan_Status.map({'Y': 1, 'N': 0})\n",
    "df.Dependents = df.Dependents.map({'0': 0.0, '1': 1.0, '2': 2.0, '3+': 3.0})\n",
    "df.ApplicantIncome = df.ApplicantIncome.astype(\"float64\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task3 split the dataset into X_train, X_test, y_train, y_test\n",
    "# Optional: you can also use normalization\n",
    "df = df.apply(lambda x: (x-np.min(x))/(np.max(x)-np.min(x)))\n",
    "array = np.array(df)\n",
    "n = array.shape[0]\n",
    "train_index = set(np.random.choice(n, int(n/4*3), replace=False))\n",
    "test_index = set(range(n)) - train_index\n",
    "(train, test) = (array[np.array(list(train_index)), :],\n",
    "                 array[np.array(list(test_index)), :])\n",
    "(X_train, y_train) = (train[:, :-1], train[:, -1:])\n",
    "(X_test, y_test) = (test[:, :-1], test[:, -1:])\n",
    "X_train = np.append(X_train, np.ones((X_train.shape[0], 1)), axis=1)\n",
    "X_test = np.append(X_test, np.ones((X_test.shape[0], 1)), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "牛顿法\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.9155844155844156\n",
      "\n",
      "梯度下降法\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.9155844155844156\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from Logistic import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "# Task4 train your model and plot the loss curve of training\n",
    "print('牛顿法')\n",
    "model = LogisticRegression(gamma=0)\n",
    "model.fit(X_train, y_train,pattern='newton')\n",
    "result = model.predict(X_test)\n",
    "delta = result-y_test\n",
    "num = 0\n",
    "for i in range(delta.shape[0]):\n",
    "    if delta[i] != 0:\n",
    "        num += 1\n",
    "print('accuracy:{}\\n'.format(1-num/delta.shape[0]))\n",
    "\n",
    "print('梯度下降法')\n",
    "model = LogisticRegression(gamma=0)\n",
    "model.fit(X_train, y_train,pattern='gd')\n",
    "result = model.predict(X_test)\n",
    "delta = result-y_test\n",
    "num = 0\n",
    "for i in range(delta.shape[0]):\n",
    "    if delta[i] != 0:\n",
    "        num += 1\n",
    "print('accuracy:{}\\n'.format(1-num/delta.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task5 compare the accuracy(or other metrics you want) of test data with different parameters you train with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8153594771241834\n"
     ]
    }
   ],
   "source": [
    "# 梯度下降法(步长0.005)\n",
    "from Logistic import LogisticRegression\n",
    "np.random.seed(37)\n",
    "df = df.apply(lambda x: (x-np.min(x))/(np.max(x)-np.min(x)))\n",
    "positive = np.array(df.iloc[(df.iloc[:, 11] == 1.0).values, :])\n",
    "negative = np.array(df.iloc[(~(df.iloc[:, 11] == 1.0)).values, :])\n",
    "np.random.shuffle(positive)\n",
    "np.random.shuffle(negative)\n",
    "pn = positive.shape[0]\n",
    "nn = negative.shape[0]\n",
    "acc = []\n",
    "model = LogisticRegression(gamma=0, plot=False)\n",
    "for i in range(10):\n",
    "    for i in range(4):\n",
    "        test_index_p = set(range(i*int(pn/4), (i+1)*int(pn/4)))\n",
    "        train_index_p = set(range(pn)) - test_index_p\n",
    "        (train_p, test_p) = (positive[np.array(list(train_index_p)), :],\n",
    "                             positive[np.array(list(test_index_p)), :])\n",
    "        test_index_n = set(range(i*int(nn/4), (i+1)*int(nn/4)))\n",
    "        train_index_n = set(range(nn)) - test_index_n\n",
    "        (train_n, test_n) = (negative[np.array(list(train_index_n)), :],\n",
    "                             negative[np.array(list(test_index_n)), :])\n",
    "        (train, test) = (np.r_[train_p, train_n], np.r_[test_p, test_n])\n",
    "        (X_train, y_train) = (train[:, :-1], train[:, -1:])\n",
    "        (X_test, y_test) = (test[:, :-1], test[:, -1:])\n",
    "        X_train = np.append(X_train, np.ones((X_train.shape[0], 1)), axis=1)\n",
    "        X_test = np.append(X_test, np.ones((X_test.shape[0], 1)), axis=1)\n",
    "        model.fit(X_train, y_train, lr=0.005)\n",
    "        result = model.predict(X_test)\n",
    "        delta = result-y_test\n",
    "        num = 0\n",
    "        for i in range(delta.shape[0]):\n",
    "            if delta[i] == 0:\n",
    "                num += 1\n",
    "        acc.append(num / delta.shape[0])\n",
    "print(sum(acc)/len(acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8153594771241834\n"
     ]
    }
   ],
   "source": [
    "# 梯度下降法(步长0.001)\n",
    "from Logistic import LogisticRegression\n",
    "np.random.seed(37)\n",
    "df = df.apply(lambda x: (x-np.min(x))/(np.max(x)-np.min(x)))\n",
    "positive = np.array(df.iloc[(df.iloc[:, 11] == 1.0).values, :])\n",
    "negative = np.array(df.iloc[(~(df.iloc[:, 11] == 1.0)).values, :])\n",
    "np.random.shuffle(positive)\n",
    "np.random.shuffle(negative)\n",
    "pn = positive.shape[0]\n",
    "nn = negative.shape[0]\n",
    "acc = []\n",
    "model = LogisticRegression(gamma=0, plot=False)\n",
    "for i in range(10):\n",
    "    for i in range(4):\n",
    "        test_index_p = set(range(i*int(pn/4), (i+1)*int(pn/4)))\n",
    "        train_index_p = set(range(pn)) - test_index_p\n",
    "        (train_p, test_p) = (positive[np.array(list(train_index_p)), :],\n",
    "                             positive[np.array(list(test_index_p)), :])\n",
    "        test_index_n = set(range(i*int(nn/4), (i+1)*int(nn/4)))\n",
    "        train_index_n = set(range(nn)) - test_index_n\n",
    "        (train_n, test_n) = (negative[np.array(list(train_index_n)), :],\n",
    "                             negative[np.array(list(test_index_n)), :])\n",
    "        (train, test) = (np.r_[train_p, train_n], np.r_[test_p, test_n])\n",
    "        (X_train, y_train) = (train[:, :-1], train[:, -1:])\n",
    "        (X_test, y_test) = (test[:, :-1], test[:, -1:])\n",
    "        X_train = np.append(X_train, np.ones((X_train.shape[0], 1)), axis=1)\n",
    "        X_test = np.append(X_test, np.ones((X_test.shape[0], 1)), axis=1)\n",
    "        model.fit(X_train, y_train, lr=0.001)\n",
    "        result = model.predict(X_test)\n",
    "        delta = result-y_test\n",
    "        num = 0\n",
    "        for i in range(delta.shape[0]):\n",
    "            if delta[i] == 0:\n",
    "                num += 1\n",
    "        acc.append(num / delta.shape[0])\n",
    "print(sum(acc)/len(acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8120915032679743\n"
     ]
    }
   ],
   "source": [
    "# 梯度下降法(步长0.005,l1正则化(参数2))\n",
    "from Logistic import LogisticRegression\n",
    "np.random.seed(37)\n",
    "df = df.apply(lambda x: (x-np.min(x))/(np.max(x)-np.min(x)))\n",
    "positive = np.array(df.iloc[(df.iloc[:, 11] == 1.0).values, :])\n",
    "negative = np.array(df.iloc[(~(df.iloc[:, 11] == 1.0)).values, :])\n",
    "np.random.shuffle(positive)\n",
    "np.random.shuffle(negative)\n",
    "pn = positive.shape[0]\n",
    "nn = negative.shape[0]\n",
    "acc = []\n",
    "model = LogisticRegression(penalty='l1',gamma=2,plot=False)\n",
    "for i in range(10):\n",
    "    for i in range(4):\n",
    "        test_index_p = set(range(i*int(pn/4), (i+1)*int(pn/4)))\n",
    "        train_index_p = set(range(pn)) - test_index_p\n",
    "        (train_p, test_p) = (positive[np.array(list(train_index_p)), :],\n",
    "                             positive[np.array(list(test_index_p)), :])\n",
    "        test_index_n = set(range(i*int(nn/4), (i+1)*int(nn/4)))\n",
    "        train_index_n = set(range(nn)) - test_index_n\n",
    "        (train_n, test_n) = (negative[np.array(list(train_index_n)), :],\n",
    "                             negative[np.array(list(test_index_n)), :])\n",
    "        (train, test) = (np.r_[train_p, train_n], np.r_[test_p, test_n])\n",
    "        (X_train, y_train) = (train[:, :-1], train[:, -1:])\n",
    "        (X_test, y_test) = (test[:, :-1], test[:, -1:])\n",
    "        X_train = np.append(X_train, np.ones((X_train.shape[0], 1)), axis=1)\n",
    "        X_test = np.append(X_test, np.ones((X_test.shape[0], 1)), axis=1)\n",
    "        model.fit(X_train, y_train, lr=0.005)\n",
    "        result = model.predict(X_test)\n",
    "        delta = result-y_test\n",
    "        num = 0\n",
    "        for i in range(delta.shape[0]):\n",
    "            if delta[i] == 0:\n",
    "                num += 1\n",
    "        acc.append(num / delta.shape[0])\n",
    "print(sum(acc)/len(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8153594771241834\n"
     ]
    }
   ],
   "source": [
    "# 梯度下降法(步长0.005,l1正则化(参数1))\n",
    "from Logistic import LogisticRegression\n",
    "np.random.seed(37)\n",
    "df = df.apply(lambda x: (x-np.min(x))/(np.max(x)-np.min(x)))\n",
    "positive = np.array(df.iloc[(df.iloc[:, 11] == 1.0).values, :])\n",
    "negative = np.array(df.iloc[(~(df.iloc[:, 11] == 1.0)).values, :])\n",
    "np.random.shuffle(positive)\n",
    "np.random.shuffle(negative)\n",
    "pn = positive.shape[0]\n",
    "nn = negative.shape[0]\n",
    "acc = []\n",
    "model = LogisticRegression(penalty='l1', plot=False)\n",
    "for i in range(10):\n",
    "    for i in range(4):\n",
    "        test_index_p = set(range(i*int(pn/4), (i+1)*int(pn/4)))\n",
    "        train_index_p = set(range(pn)) - test_index_p\n",
    "        (train_p, test_p) = (positive[np.array(list(train_index_p)), :],\n",
    "                             positive[np.array(list(test_index_p)), :])\n",
    "        test_index_n = set(range(i*int(nn/4), (i+1)*int(nn/4)))\n",
    "        train_index_n = set(range(nn)) - test_index_n\n",
    "        (train_n, test_n) = (negative[np.array(list(train_index_n)), :],\n",
    "                             negative[np.array(list(test_index_n)), :])\n",
    "        (train, test) = (np.r_[train_p, train_n], np.r_[test_p, test_n])\n",
    "        (X_train, y_train) = (train[:, :-1], train[:, -1:])\n",
    "        (X_test, y_test) = (test[:, :-1], test[:, -1:])\n",
    "        X_train = np.append(X_train, np.ones((X_train.shape[0], 1)), axis=1)\n",
    "        X_test = np.append(X_test, np.ones((X_test.shape[0], 1)), axis=1)\n",
    "        model.fit(X_train, y_train, lr=0.005)\n",
    "        result = model.predict(X_test)\n",
    "        delta = result-y_test\n",
    "        num = 0\n",
    "        for i in range(delta.shape[0]):\n",
    "            if delta[i] == 0:\n",
    "                num += 1\n",
    "        acc.append(num / delta.shape[0])\n",
    "print(sum(acc)/len(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8153594771241834\n"
     ]
    }
   ],
   "source": [
    "# 梯度下降法(步长0.005,l1正则化(参数0.5))\n",
    "from Logistic import LogisticRegression\n",
    "np.random.seed(37)\n",
    "df = df.apply(lambda x: (x-np.min(x))/(np.max(x)-np.min(x)))\n",
    "positive = np.array(df.iloc[(df.iloc[:, 11] == 1.0).values, :])\n",
    "negative = np.array(df.iloc[(~(df.iloc[:, 11] == 1.0)).values, :])\n",
    "np.random.shuffle(positive)\n",
    "np.random.shuffle(negative)\n",
    "pn = positive.shape[0]\n",
    "nn = negative.shape[0]\n",
    "acc = []\n",
    "model = LogisticRegression(penalty='l1',gamma=0.5,plot=False)\n",
    "for i in range(10):\n",
    "    for i in range(4):\n",
    "        test_index_p = set(range(i*int(pn/4), (i+1)*int(pn/4)))\n",
    "        train_index_p = set(range(pn)) - test_index_p\n",
    "        (train_p, test_p) = (positive[np.array(list(train_index_p)), :],\n",
    "                             positive[np.array(list(test_index_p)), :])\n",
    "        test_index_n = set(range(i*int(nn/4), (i+1)*int(nn/4)))\n",
    "        train_index_n = set(range(nn)) - test_index_n\n",
    "        (train_n, test_n) = (negative[np.array(list(train_index_n)), :],\n",
    "                             negative[np.array(list(test_index_n)), :])\n",
    "        (train, test) = (np.r_[train_p, train_n], np.r_[test_p, test_n])\n",
    "        (X_train, y_train) = (train[:, :-1], train[:, -1:])\n",
    "        (X_test, y_test) = (test[:, :-1], test[:, -1:])\n",
    "        X_train = np.append(X_train, np.ones((X_train.shape[0], 1)), axis=1)\n",
    "        X_test = np.append(X_test, np.ones((X_test.shape[0], 1)), axis=1)\n",
    "        model.fit(X_train, y_train, lr=0.005)\n",
    "        result = model.predict(X_test)\n",
    "        delta = result-y_test\n",
    "        num = 0\n",
    "        for i in range(delta.shape[0]):\n",
    "            if delta[i] == 0:\n",
    "                num += 1\n",
    "        acc.append(num / delta.shape[0])\n",
    "print(sum(acc)/len(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8137254901960789\n"
     ]
    }
   ],
   "source": [
    "# 梯度下降法(步长0.005,l2正则化(参数2))\n",
    "from Logistic import LogisticRegression\n",
    "np.random.seed(37)\n",
    "df = df.apply(lambda x: (x-np.min(x))/(np.max(x)-np.min(x)))\n",
    "positive = np.array(df.iloc[(df.iloc[:, 11] == 1.0).values, :])\n",
    "negative = np.array(df.iloc[(~(df.iloc[:, 11] == 1.0)).values, :])\n",
    "np.random.shuffle(positive)\n",
    "np.random.shuffle(negative)\n",
    "pn = positive.shape[0]\n",
    "nn = negative.shape[0]\n",
    "acc = []\n",
    "model = LogisticRegression(penalty='l2',gamma=2, plot=False)\n",
    "for i in range(10):\n",
    "    for i in range(4):\n",
    "        test_index_p = set(range(i*int(pn/4), (i+1)*int(pn/4)))\n",
    "        train_index_p = set(range(pn)) - test_index_p\n",
    "        (train_p, test_p) = (positive[np.array(list(train_index_p)), :],\n",
    "                             positive[np.array(list(test_index_p)), :])\n",
    "        test_index_n = set(range(i*int(nn/4), (i+1)*int(nn/4)))\n",
    "        train_index_n = set(range(nn)) - test_index_n\n",
    "        (train_n, test_n) = (negative[np.array(list(train_index_n)), :],\n",
    "                             negative[np.array(list(test_index_n)), :])\n",
    "        (train, test) = (np.r_[train_p, train_n], np.r_[test_p, test_n])\n",
    "        (X_train, y_train) = (train[:, :-1], train[:, -1:])\n",
    "        (X_test, y_test) = (test[:, :-1], test[:, -1:])\n",
    "        X_train = np.append(X_train, np.ones((X_train.shape[0], 1)), axis=1)\n",
    "        X_test = np.append(X_test, np.ones((X_test.shape[0], 1)), axis=1)\n",
    "        model.fit(X_train, y_train, lr=0.005)\n",
    "        result = model.predict(X_test)\n",
    "        delta = result-y_test\n",
    "        num = 0\n",
    "        for i in range(delta.shape[0]):\n",
    "            if delta[i] == 0:\n",
    "                num += 1\n",
    "        acc.append(num / delta.shape[0])\n",
    "print(sum(acc)/len(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8137254901960789\n"
     ]
    }
   ],
   "source": [
    "# 梯度下降法(步长0.005,l2正则化(参数1))\n",
    "from Logistic import LogisticRegression\n",
    "np.random.seed(37)\n",
    "df = df.apply(lambda x: (x-np.min(x))/(np.max(x)-np.min(x)))\n",
    "positive = np.array(df.iloc[(df.iloc[:, 11] == 1.0).values, :])\n",
    "negative = np.array(df.iloc[(~(df.iloc[:, 11] == 1.0)).values, :])\n",
    "np.random.shuffle(positive)\n",
    "np.random.shuffle(negative)\n",
    "pn = positive.shape[0]\n",
    "nn = negative.shape[0]\n",
    "acc = []\n",
    "model = LogisticRegression(penalty='l2',plot=False)\n",
    "for i in range(10):\n",
    "    for i in range(4):\n",
    "        test_index_p = set(range(i*int(pn/4), (i+1)*int(pn/4)))\n",
    "        train_index_p = set(range(pn)) - test_index_p\n",
    "        (train_p, test_p) = (positive[np.array(list(train_index_p)), :],\n",
    "                             positive[np.array(list(test_index_p)), :])\n",
    "        test_index_n = set(range(i*int(nn/4), (i+1)*int(nn/4)))\n",
    "        train_index_n = set(range(nn)) - test_index_n\n",
    "        (train_n, test_n) = (negative[np.array(list(train_index_n)), :],\n",
    "                             negative[np.array(list(test_index_n)), :])\n",
    "        (train, test) = (np.r_[train_p, train_n], np.r_[test_p, test_n])\n",
    "        (X_train, y_train) = (train[:, :-1], train[:, -1:])\n",
    "        (X_test, y_test) = (test[:, :-1], test[:, -1:])\n",
    "        X_train = np.append(X_train, np.ones((X_train.shape[0], 1)), axis=1)\n",
    "        X_test = np.append(X_test, np.ones((X_test.shape[0], 1)), axis=1)\n",
    "        model.fit(X_train, y_train, lr=0.005)\n",
    "        result = model.predict(X_test)\n",
    "        delta = result-y_test\n",
    "        num = 0\n",
    "        for i in range(delta.shape[0]):\n",
    "            if delta[i] == 0:\n",
    "                num += 1\n",
    "        acc.append(num / delta.shape[0])\n",
    "print(sum(acc)/len(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8153594771241834\n"
     ]
    }
   ],
   "source": [
    "# 梯度下降法(步长0.005,l2正则化(参数0.5))\n",
    "from Logistic import LogisticRegression\n",
    "np.random.seed(37)\n",
    "df = df.apply(lambda x: (x-np.min(x))/(np.max(x)-np.min(x)))\n",
    "positive = np.array(df.iloc[(df.iloc[:, 11] == 1.0).values, :])\n",
    "negative = np.array(df.iloc[(~(df.iloc[:, 11] == 1.0)).values, :])\n",
    "np.random.shuffle(positive)\n",
    "np.random.shuffle(negative)\n",
    "pn = positive.shape[0]\n",
    "nn = negative.shape[0]\n",
    "acc = []\n",
    "model = LogisticRegression(penalty='l2',gamma=0.5, plot=False)\n",
    "for i in range(10):\n",
    "    for i in range(4):\n",
    "        test_index_p = set(range(i*int(pn/4), (i+1)*int(pn/4)))\n",
    "        train_index_p = set(range(pn)) - test_index_p\n",
    "        (train_p, test_p) = (positive[np.array(list(train_index_p)), :],\n",
    "                             positive[np.array(list(test_index_p)), :])\n",
    "        test_index_n = set(range(i*int(nn/4), (i+1)*int(nn/4)))\n",
    "        train_index_n = set(range(nn)) - test_index_n\n",
    "        (train_n, test_n) = (negative[np.array(list(train_index_n)), :],\n",
    "                             negative[np.array(list(test_index_n)), :])\n",
    "        (train, test) = (np.r_[train_p, train_n], np.r_[test_p, test_n])\n",
    "        (X_train, y_train) = (train[:, :-1], train[:, -1:])\n",
    "        (X_test, y_test) = (test[:, :-1], test[:, -1:])\n",
    "        X_train = np.append(X_train, np.ones((X_train.shape[0], 1)), axis=1)\n",
    "        X_test = np.append(X_test, np.ones((X_test.shape[0], 1)), axis=1)\n",
    "        model.fit(X_train, y_train, lr=0.005)\n",
    "        result = model.predict(X_test)\n",
    "        delta = result-y_test\n",
    "        num = 0\n",
    "        for i in range(delta.shape[0]):\n",
    "            if delta[i] == 0:\n",
    "                num += 1\n",
    "        acc.append(num / delta.shape[0])\n",
    "print(sum(acc)/len(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8153594771241834\n"
     ]
    }
   ],
   "source": [
    "# 梯度下降法(步长0.005,tol=1e-5)\n",
    "from Logistic import LogisticRegression\n",
    "np.random.seed(37)\n",
    "df = df.apply(lambda x: (x-np.min(x))/(np.max(x)-np.min(x)))\n",
    "positive = np.array(df.iloc[(df.iloc[:, 11] == 1.0).values, :])\n",
    "negative = np.array(df.iloc[(~(df.iloc[:, 11] == 1.0)).values, :])\n",
    "np.random.shuffle(positive)\n",
    "np.random.shuffle(negative)\n",
    "pn = positive.shape[0]\n",
    "nn = negative.shape[0]\n",
    "acc = []\n",
    "model = LogisticRegression(gamma=0, plot=False)\n",
    "for i in range(10):\n",
    "    for i in range(4):\n",
    "        test_index_p = set(range(i*int(pn/4), (i+1)*int(pn/4)))\n",
    "        train_index_p = set(range(pn)) - test_index_p\n",
    "        (train_p, test_p) = (positive[np.array(list(train_index_p)), :],\n",
    "                             positive[np.array(list(test_index_p)), :])\n",
    "        test_index_n = set(range(i*int(nn/4), (i+1)*int(nn/4)))\n",
    "        train_index_n = set(range(nn)) - test_index_n\n",
    "        (train_n, test_n) = (negative[np.array(list(train_index_n)), :],\n",
    "                             negative[np.array(list(test_index_n)), :])\n",
    "        (train, test) = (np.r_[train_p, train_n], np.r_[test_p, test_n])\n",
    "        (X_train, y_train) = (train[:, :-1], train[:, -1:])\n",
    "        (X_test, y_test) = (test[:, :-1], test[:, -1:])\n",
    "        X_train = np.append(X_train, np.ones((X_train.shape[0], 1)), axis=1)\n",
    "        X_test = np.append(X_test, np.ones((X_test.shape[0], 1)), axis=1)\n",
    "        model.fit(X_train, y_train, lr=0.005, tol=1e-5)\n",
    "        result = model.predict(X_test)\n",
    "        delta = result-y_test\n",
    "        num = 0\n",
    "        for i in range(delta.shape[0]):\n",
    "            if delta[i] == 0:\n",
    "                num += 1\n",
    "        acc.append(num / delta.shape[0])\n",
    "print(sum(acc)/len(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8137254901960789\n"
     ]
    }
   ],
   "source": [
    "# 梯度下降法(步长0.005,tol=1e-3)\n",
    "from Logistic import LogisticRegression\n",
    "np.random.seed(37)\n",
    "df = df.apply(lambda x: (x-np.min(x))/(np.max(x)-np.min(x)))\n",
    "positive = np.array(df.iloc[(df.iloc[:, 11] == 1.0).values, :])\n",
    "negative = np.array(df.iloc[(~(df.iloc[:, 11] == 1.0)).values, :])\n",
    "np.random.shuffle(positive)\n",
    "np.random.shuffle(negative)\n",
    "pn = positive.shape[0]\n",
    "nn = negative.shape[0]\n",
    "acc = []\n",
    "model = LogisticRegression(gamma=0, plot=False)\n",
    "for i in range(10):\n",
    "    for i in range(4):\n",
    "        test_index_p = set(range(i*int(pn/4), (i+1)*int(pn/4)))\n",
    "        train_index_p = set(range(pn)) - test_index_p\n",
    "        (train_p, test_p) = (positive[np.array(list(train_index_p)), :],\n",
    "                             positive[np.array(list(test_index_p)), :])\n",
    "        test_index_n = set(range(i*int(nn/4), (i+1)*int(nn/4)))\n",
    "        train_index_n = set(range(nn)) - test_index_n\n",
    "        (train_n, test_n) = (negative[np.array(list(train_index_n)), :],\n",
    "                             negative[np.array(list(test_index_n)), :])\n",
    "        (train, test) = (np.r_[train_p, train_n], np.r_[test_p, test_n])\n",
    "        (X_train, y_train) = (train[:, :-1], train[:, -1:])\n",
    "        (X_test, y_test) = (test[:, :-1], test[:, -1:])\n",
    "        X_train = np.append(X_train, np.ones((X_train.shape[0], 1)), axis=1)\n",
    "        X_test = np.append(X_test, np.ones((X_test.shape[0], 1)), axis=1)\n",
    "        model.fit(X_train, y_train, lr=0.005, tol=1e-3)\n",
    "        result = model.predict(X_test)\n",
    "        delta = result-y_test\n",
    "        num = 0\n",
    "        for i in range(delta.shape[0]):\n",
    "            if delta[i] == 0:\n",
    "                num += 1\n",
    "        acc.append(num / delta.shape[0])\n",
    "print(sum(acc)/len(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8153594771241834\n"
     ]
    }
   ],
   "source": [
    "# 牛顿法\n",
    "from Logistic import LogisticRegression\n",
    "np.random.seed(37)\n",
    "df = df.apply(lambda x: (x-np.min(x))/(np.max(x)-np.min(x)))\n",
    "positive = np.array(df.iloc[(df.iloc[:, 11] == 1.0).values, :])\n",
    "negative = np.array(df.iloc[(~(df.iloc[:, 11] == 1.0)).values, :])\n",
    "np.random.shuffle(positive)\n",
    "np.random.shuffle(negative)\n",
    "pn = positive.shape[0]\n",
    "nn = negative.shape[0]\n",
    "acc = []\n",
    "model = LogisticRegression(gamma=0, plot=False)\n",
    "for i in range(10):\n",
    "    for i in range(4):\n",
    "        test_index_p = set(range(i*int(pn/4), (i+1)*int(pn/4)))\n",
    "        train_index_p = set(range(pn)) - test_index_p\n",
    "        (train_p, test_p) = (positive[np.array(list(train_index_p)), :],\n",
    "                             positive[np.array(list(test_index_p)), :])\n",
    "        test_index_n = set(range(i*int(nn/4), (i+1)*int(nn/4)))\n",
    "        train_index_n = set(range(nn)) - test_index_n\n",
    "        (train_n, test_n) = (negative[np.array(list(train_index_n)), :],\n",
    "                             negative[np.array(list(test_index_n)), :])\n",
    "        (train, test) = (np.r_[train_p, train_n], np.r_[test_p, test_n])\n",
    "        (X_train, y_train) = (train[:, :-1], train[:, -1:])\n",
    "        (X_test, y_test) = (test[:, :-1], test[:, -1:])\n",
    "        X_train = np.append(X_train, np.ones((X_train.shape[0], 1)), axis=1)\n",
    "        X_test = np.append(X_test, np.ones((X_test.shape[0], 1)), axis=1)\n",
    "        model.fit(X_train, y_train, pattern='newton', lr=1)\n",
    "        result = model.predict(X_test)\n",
    "        delta = result-y_test\n",
    "        num = 0\n",
    "        for i in range(delta.shape[0]):\n",
    "            if delta[i] == 0:\n",
    "                num += 1\n",
    "        acc.append(num / delta.shape[0])\n",
    "print(sum(acc)/len(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8137254901960789\n"
     ]
    }
   ],
   "source": [
    "# 牛顿法(l1正则化(参数2))\n",
    "from Logistic import LogisticRegression\n",
    "np.random.seed(37)\n",
    "df = df.apply(lambda x: (x-np.min(x))/(np.max(x)-np.min(x)))\n",
    "positive = np.array(df.iloc[(df.iloc[:, 11] == 1.0).values, :])\n",
    "negative = np.array(df.iloc[(~(df.iloc[:, 11] == 1.0)).values, :])\n",
    "np.random.shuffle(positive)\n",
    "np.random.shuffle(negative)\n",
    "pn = positive.shape[0]\n",
    "nn = negative.shape[0]\n",
    "acc = []\n",
    "model = LogisticRegression(penalty='l1',gamma=2, plot=False)\n",
    "for i in range(10):\n",
    "    for i in range(4):\n",
    "        test_index_p = set(range(i*int(pn/4), (i+1)*int(pn/4)))\n",
    "        train_index_p = set(range(pn)) - test_index_p\n",
    "        (train_p, test_p) = (positive[np.array(list(train_index_p)), :],\n",
    "                             positive[np.array(list(test_index_p)), :])\n",
    "        test_index_n = set(range(i*int(nn/4), (i+1)*int(nn/4)))\n",
    "        train_index_n = set(range(nn)) - test_index_n\n",
    "        (train_n, test_n) = (negative[np.array(list(train_index_n)), :],\n",
    "                             negative[np.array(list(test_index_n)), :])\n",
    "        (train, test) = (np.r_[train_p, train_n], np.r_[test_p, test_n])\n",
    "        (X_train, y_train) = (train[:, :-1], train[:, -1:])\n",
    "        (X_test, y_test) = (test[:, :-1], test[:, -1:])\n",
    "        X_train = np.append(X_train, np.ones((X_train.shape[0], 1)), axis=1)\n",
    "        X_test = np.append(X_test, np.ones((X_test.shape[0], 1)), axis=1)\n",
    "        model.fit(X_train, y_train, pattern='newton', lr=1, tol=1e-1)\n",
    "        result = model.predict(X_test)\n",
    "        delta = result-y_test\n",
    "        num = 0\n",
    "        for i in range(delta.shape[0]):\n",
    "            if delta[i] == 0:\n",
    "                num += 1\n",
    "        acc.append(num / delta.shape[0])\n",
    "print(sum(acc)/len(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8153594771241834\n"
     ]
    }
   ],
   "source": [
    "# 牛顿法(l1正则化(参数1))\n",
    "from Logistic import LogisticRegression\n",
    "np.random.seed(37)\n",
    "df = df.apply(lambda x: (x-np.min(x))/(np.max(x)-np.min(x)))\n",
    "positive = np.array(df.iloc[(df.iloc[:, 11] == 1.0).values, :])\n",
    "negative = np.array(df.iloc[(~(df.iloc[:, 11] == 1.0)).values, :])\n",
    "np.random.shuffle(positive)\n",
    "np.random.shuffle(negative)\n",
    "pn = positive.shape[0]\n",
    "nn = negative.shape[0]\n",
    "acc = []\n",
    "model = LogisticRegression(penalty='l1', plot=False)\n",
    "for i in range(10):\n",
    "    for i in range(4):\n",
    "        test_index_p = set(range(i*int(pn/4), (i+1)*int(pn/4)))\n",
    "        train_index_p = set(range(pn)) - test_index_p\n",
    "        (train_p, test_p) = (positive[np.array(list(train_index_p)), :],\n",
    "                             positive[np.array(list(test_index_p)), :])\n",
    "        test_index_n = set(range(i*int(nn/4), (i+1)*int(nn/4)))\n",
    "        train_index_n = set(range(nn)) - test_index_n\n",
    "        (train_n, test_n) = (negative[np.array(list(train_index_n)), :],\n",
    "                             negative[np.array(list(test_index_n)), :])\n",
    "        (train, test) = (np.r_[train_p, train_n], np.r_[test_p, test_n])\n",
    "        (X_train, y_train) = (train[:, :-1], train[:, -1:])\n",
    "        (X_test, y_test) = (test[:, :-1], test[:, -1:])\n",
    "        X_train = np.append(X_train, np.ones((X_train.shape[0], 1)), axis=1)\n",
    "        X_test = np.append(X_test, np.ones((X_test.shape[0], 1)), axis=1)\n",
    "        model.fit(X_train, y_train, pattern='newton', lr=1, tol=1e-1)\n",
    "        result = model.predict(X_test)\n",
    "        delta = result-y_test\n",
    "        num = 0\n",
    "        for i in range(delta.shape[0]):\n",
    "            if delta[i] == 0:\n",
    "                num += 1\n",
    "        acc.append(num / delta.shape[0])\n",
    "print(sum(acc)/len(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8153594771241834\n"
     ]
    }
   ],
   "source": [
    "# 牛顿法(l1正则化(参数0.5))\n",
    "from Logistic import LogisticRegression\n",
    "np.random.seed(37)\n",
    "df = df.apply(lambda x: (x-np.min(x))/(np.max(x)-np.min(x)))\n",
    "positive = np.array(df.iloc[(df.iloc[:, 11] == 1.0).values, :])\n",
    "negative = np.array(df.iloc[(~(df.iloc[:, 11] == 1.0)).values, :])\n",
    "np.random.shuffle(positive)\n",
    "np.random.shuffle(negative)\n",
    "pn = positive.shape[0]\n",
    "nn = negative.shape[0]\n",
    "acc = []\n",
    "model = LogisticRegression(penalty='l1',gamma=0.5, plot=False)\n",
    "for i in range(10):\n",
    "    for i in range(4):\n",
    "        test_index_p = set(range(i*int(pn/4), (i+1)*int(pn/4)))\n",
    "        train_index_p = set(range(pn)) - test_index_p\n",
    "        (train_p, test_p) = (positive[np.array(list(train_index_p)), :],\n",
    "                             positive[np.array(list(test_index_p)), :])\n",
    "        test_index_n = set(range(i*int(nn/4), (i+1)*int(nn/4)))\n",
    "        train_index_n = set(range(nn)) - test_index_n\n",
    "        (train_n, test_n) = (negative[np.array(list(train_index_n)), :],\n",
    "                             negative[np.array(list(test_index_n)), :])\n",
    "        (train, test) = (np.r_[train_p, train_n], np.r_[test_p, test_n])\n",
    "        (X_train, y_train) = (train[:, :-1], train[:, -1:])\n",
    "        (X_test, y_test) = (test[:, :-1], test[:, -1:])\n",
    "        X_train = np.append(X_train, np.ones((X_train.shape[0], 1)), axis=1)\n",
    "        X_test = np.append(X_test, np.ones((X_test.shape[0], 1)), axis=1)\n",
    "        model.fit(X_train, y_train, pattern='newton', lr=1)\n",
    "        result = model.predict(X_test)\n",
    "        delta = result-y_test\n",
    "        num = 0\n",
    "        for i in range(delta.shape[0]):\n",
    "            if delta[i] == 0:\n",
    "                num += 1\n",
    "        acc.append(num / delta.shape[0])\n",
    "print(sum(acc)/len(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8137254901960789\n"
     ]
    }
   ],
   "source": [
    "# 牛顿法(l2正则化(参数2))\n",
    "from Logistic import LogisticRegression\n",
    "np.random.seed(37)\n",
    "df = df.apply(lambda x: (x-np.min(x))/(np.max(x)-np.min(x)))\n",
    "positive = np.array(df.iloc[(df.iloc[:, 11] == 1.0).values, :])\n",
    "negative = np.array(df.iloc[(~(df.iloc[:, 11] == 1.0)).values, :])\n",
    "np.random.shuffle(positive)\n",
    "np.random.shuffle(negative)\n",
    "pn = positive.shape[0]\n",
    "nn = negative.shape[0]\n",
    "acc = []\n",
    "model = LogisticRegression(penalty='l2',gamma=2, plot=False)\n",
    "for i in range(10):\n",
    "    for i in range(4):\n",
    "        test_index_p = set(range(i*int(pn/4), (i+1)*int(pn/4)))\n",
    "        train_index_p = set(range(pn)) - test_index_p\n",
    "        (train_p, test_p) = (positive[np.array(list(train_index_p)), :],\n",
    "                             positive[np.array(list(test_index_p)), :])\n",
    "        test_index_n = set(range(i*int(nn/4), (i+1)*int(nn/4)))\n",
    "        train_index_n = set(range(nn)) - test_index_n\n",
    "        (train_n, test_n) = (negative[np.array(list(train_index_n)), :],\n",
    "                             negative[np.array(list(test_index_n)), :])\n",
    "        (train, test) = (np.r_[train_p, train_n], np.r_[test_p, test_n])\n",
    "        (X_train, y_train) = (train[:, :-1], train[:, -1:])\n",
    "        (X_test, y_test) = (test[:, :-1], test[:, -1:])\n",
    "        X_train = np.append(X_train, np.ones((X_train.shape[0], 1)), axis=1)\n",
    "        X_test = np.append(X_test, np.ones((X_test.shape[0], 1)), axis=1)\n",
    "        model.fit(X_train, y_train, pattern='newton', lr=1)\n",
    "        result = model.predict(X_test)\n",
    "        delta = result-y_test\n",
    "        num = 0\n",
    "        for i in range(delta.shape[0]):\n",
    "            if delta[i] == 0:\n",
    "                num += 1\n",
    "        acc.append(num / delta.shape[0])\n",
    "print(sum(acc)/len(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8137254901960789\n"
     ]
    }
   ],
   "source": [
    "# 牛顿法(l2正则化(参数1))\n",
    "from Logistic import LogisticRegression\n",
    "np.random.seed(37)\n",
    "df = df.apply(lambda x: (x-np.min(x))/(np.max(x)-np.min(x)))\n",
    "positive = np.array(df.iloc[(df.iloc[:, 11] == 1.0).values, :])\n",
    "negative = np.array(df.iloc[(~(df.iloc[:, 11] == 1.0)).values, :])\n",
    "np.random.shuffle(positive)\n",
    "np.random.shuffle(negative)\n",
    "pn = positive.shape[0]\n",
    "nn = negative.shape[0]\n",
    "acc = []\n",
    "model = LogisticRegression(penalty='l2', plot=False)\n",
    "for i in range(10):\n",
    "    for i in range(4):\n",
    "        test_index_p = set(range(i*int(pn/4), (i+1)*int(pn/4)))\n",
    "        train_index_p = set(range(pn)) - test_index_p\n",
    "        (train_p, test_p) = (positive[np.array(list(train_index_p)), :],\n",
    "                             positive[np.array(list(test_index_p)), :])\n",
    "        test_index_n = set(range(i*int(nn/4), (i+1)*int(nn/4)))\n",
    "        train_index_n = set(range(nn)) - test_index_n\n",
    "        (train_n, test_n) = (negative[np.array(list(train_index_n)), :],\n",
    "                             negative[np.array(list(test_index_n)), :])\n",
    "        (train, test) = (np.r_[train_p, train_n], np.r_[test_p, test_n])\n",
    "        (X_train, y_train) = (train[:, :-1], train[:, -1:])\n",
    "        (X_test, y_test) = (test[:, :-1], test[:, -1:])\n",
    "        X_train = np.append(X_train, np.ones((X_train.shape[0], 1)), axis=1)\n",
    "        X_test = np.append(X_test, np.ones((X_test.shape[0], 1)), axis=1)\n",
    "        model.fit(X_train, y_train, pattern='newton', lr=1)\n",
    "        result = model.predict(X_test)\n",
    "        delta = result-y_test\n",
    "        num = 0\n",
    "        for i in range(delta.shape[0]):\n",
    "            if delta[i] == 0:\n",
    "                num += 1\n",
    "        acc.append(num / delta.shape[0])\n",
    "print(sum(acc)/len(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8153594771241834\n"
     ]
    }
   ],
   "source": [
    "# 牛顿法(l2正则化(参数0.5))\n",
    "from Logistic import LogisticRegression\n",
    "np.random.seed(37)\n",
    "df = df.apply(lambda x: (x-np.min(x))/(np.max(x)-np.min(x)))\n",
    "positive = np.array(df.iloc[(df.iloc[:, 11] == 1.0).values, :])\n",
    "negative = np.array(df.iloc[(~(df.iloc[:, 11] == 1.0)).values, :])\n",
    "np.random.shuffle(positive)\n",
    "np.random.shuffle(negative)\n",
    "pn = positive.shape[0]\n",
    "nn = negative.shape[0]\n",
    "acc = []\n",
    "model = LogisticRegression(penalty='l2',gamma=0.5, plot=False)\n",
    "for i in range(10):\n",
    "    for i in range(4):\n",
    "        test_index_p = set(range(i*int(pn/4), (i+1)*int(pn/4)))\n",
    "        train_index_p = set(range(pn)) - test_index_p\n",
    "        (train_p, test_p) = (positive[np.array(list(train_index_p)), :],\n",
    "                             positive[np.array(list(test_index_p)), :])\n",
    "        test_index_n = set(range(i*int(nn/4), (i+1)*int(nn/4)))\n",
    "        train_index_n = set(range(nn)) - test_index_n\n",
    "        (train_n, test_n) = (negative[np.array(list(train_index_n)), :],\n",
    "                             negative[np.array(list(test_index_n)), :])\n",
    "        (train, test) = (np.r_[train_p, train_n], np.r_[test_p, test_n])\n",
    "        (X_train, y_train) = (train[:, :-1], train[:, -1:])\n",
    "        (X_test, y_test) = (test[:, :-1], test[:, -1:])\n",
    "        X_train = np.append(X_train, np.ones((X_train.shape[0], 1)), axis=1)\n",
    "        X_test = np.append(X_test, np.ones((X_test.shape[0], 1)), axis=1)\n",
    "        model.fit(X_train, y_train, pattern='newton', lr=1)\n",
    "        result = model.predict(X_test)\n",
    "        delta = result-y_test\n",
    "        num = 0\n",
    "        for i in range(delta.shape[0]):\n",
    "            if delta[i] == 0:\n",
    "                num += 1\n",
    "        acc.append(num / delta.shape[0])\n",
    "print(sum(acc)/len(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8153594771241834\n"
     ]
    }
   ],
   "source": [
    "# 牛顿法(tol=1e-5)\n",
    "from Logistic import LogisticRegression\n",
    "np.random.seed(37)\n",
    "df = df.apply(lambda x: (x-np.min(x))/(np.max(x)-np.min(x)))\n",
    "positive = np.array(df.iloc[(df.iloc[:, 11] == 1.0).values, :])\n",
    "negative = np.array(df.iloc[(~(df.iloc[:, 11] == 1.0)).values, :])\n",
    "np.random.shuffle(positive)\n",
    "np.random.shuffle(negative)\n",
    "pn = positive.shape[0]\n",
    "nn = negative.shape[0]\n",
    "acc = []\n",
    "model = LogisticRegression(gamma=0, plot=False)\n",
    "for i in range(10):\n",
    "    for i in range(4):\n",
    "        test_index_p = set(range(i*int(pn/4), (i+1)*int(pn/4)))\n",
    "        train_index_p = set(range(pn)) - test_index_p\n",
    "        (train_p, test_p) = (positive[np.array(list(train_index_p)), :],\n",
    "                             positive[np.array(list(test_index_p)), :])\n",
    "        test_index_n = set(range(i*int(nn/4), (i+1)*int(nn/4)))\n",
    "        train_index_n = set(range(nn)) - test_index_n\n",
    "        (train_n, test_n) = (negative[np.array(list(train_index_n)), :],\n",
    "                             negative[np.array(list(test_index_n)), :])\n",
    "        (train, test) = (np.r_[train_p, train_n], np.r_[test_p, test_n])\n",
    "        (X_train, y_train) = (train[:, :-1], train[:, -1:])\n",
    "        (X_test, y_test) = (test[:, :-1], test[:, -1:])\n",
    "        X_train = np.append(X_train, np.ones((X_train.shape[0], 1)), axis=1)\n",
    "        X_test = np.append(X_test, np.ones((X_test.shape[0], 1)), axis=1)\n",
    "        model.fit(X_train, y_train, pattern='newton', lr=1,tol=1e-5)\n",
    "        result = model.predict(X_test)\n",
    "        delta = result-y_test\n",
    "        num = 0\n",
    "        for i in range(delta.shape[0]):\n",
    "            if delta[i] == 0:\n",
    "                num += 1\n",
    "        acc.append(num / delta.shape[0])\n",
    "print(sum(acc)/len(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8153594771241834\n"
     ]
    }
   ],
   "source": [
    "# 牛顿法(tol=1e-3)\n",
    "from Logistic import LogisticRegression\n",
    "np.random.seed(37)\n",
    "df = df.apply(lambda x: (x-np.min(x))/(np.max(x)-np.min(x)))\n",
    "positive = np.array(df.iloc[(df.iloc[:, 11] == 1.0).values, :])\n",
    "negative = np.array(df.iloc[(~(df.iloc[:, 11] == 1.0)).values, :])\n",
    "np.random.shuffle(positive)\n",
    "np.random.shuffle(negative)\n",
    "pn = positive.shape[0]\n",
    "nn = negative.shape[0]\n",
    "acc = []\n",
    "model = LogisticRegression(gamma=0, plot=False)\n",
    "for i in range(10):\n",
    "    for i in range(4):\n",
    "        test_index_p = set(range(i*int(pn/4), (i+1)*int(pn/4)))\n",
    "        train_index_p = set(range(pn)) - test_index_p\n",
    "        (train_p, test_p) = (positive[np.array(list(train_index_p)), :],\n",
    "                             positive[np.array(list(test_index_p)), :])\n",
    "        test_index_n = set(range(i*int(nn/4), (i+1)*int(nn/4)))\n",
    "        train_index_n = set(range(nn)) - test_index_n\n",
    "        (train_n, test_n) = (negative[np.array(list(train_index_n)), :],\n",
    "                             negative[np.array(list(test_index_n)), :])\n",
    "        (train, test) = (np.r_[train_p, train_n], np.r_[test_p, test_n])\n",
    "        (X_train, y_train) = (train[:, :-1], train[:, -1:])\n",
    "        (X_test, y_test) = (test[:, :-1], test[:, -1:])\n",
    "        X_train = np.append(X_train, np.ones((X_train.shape[0], 1)), axis=1)\n",
    "        X_test = np.append(X_test, np.ones((X_test.shape[0], 1)), axis=1)\n",
    "        model.fit(X_train, y_train, pattern='newton', lr=1,tol=1e-3)\n",
    "        result = model.predict(X_test)\n",
    "        delta = result-y_test\n",
    "        num = 0\n",
    "        for i in range(delta.shape[0]):\n",
    "            if delta[i] == 0:\n",
    "                num += 1\n",
    "        acc.append(num / delta.shape[0])\n",
    "print(sum(acc)/len(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8137254901960789\n"
     ]
    }
   ],
   "source": [
    "# 牛顿法(保留第6789列特征)\n",
    "from Logistic import LogisticRegression\n",
    "np.random.seed(37)\n",
    "df = df.apply(lambda x: (x-np.min(x))/(np.max(x)-np.min(x)))\n",
    "_df = df.iloc[:,[6,7,8,9,11]]\n",
    "positive = np.array(_df.iloc[(_df.iloc[:, 4] == 1.0).values, :])\n",
    "negative = np.array(_df.iloc[(~(_df.iloc[:, 4] == 1.0)).values, :])\n",
    "np.random.shuffle(positive)\n",
    "np.random.shuffle(negative)\n",
    "pn = positive.shape[0]\n",
    "nn = negative.shape[0]\n",
    "acc = []\n",
    "model = LogisticRegression(gamma=0, plot=False)\n",
    "for i in range(10):\n",
    "    for i in range(4):\n",
    "        test_index_p = set(range(i*int(pn/4), (i+1)*int(pn/4)))\n",
    "        train_index_p = set(range(pn)) - test_index_p\n",
    "        (train_p, test_p) = (positive[np.array(list(train_index_p)), :],\n",
    "                             positive[np.array(list(test_index_p)), :])\n",
    "        test_index_n = set(range(i*int(nn/4), (i+1)*int(nn/4)))\n",
    "        train_index_n = set(range(nn)) - test_index_n\n",
    "        (train_n, test_n) = (negative[np.array(list(train_index_n)), :],\n",
    "                             negative[np.array(list(test_index_n)), :])\n",
    "        (train, test) = (np.r_[train_p, train_n], np.r_[test_p, test_n])\n",
    "        (X_train, y_train) = (train[:, :-1], train[:, -1:])\n",
    "        (X_test, y_test) = (test[:, :-1], test[:, -1:])\n",
    "        X_train = np.append(X_train, np.ones((X_train.shape[0], 1)), axis=1)\n",
    "        X_test = np.append(X_test, np.ones((X_test.shape[0], 1)), axis=1)\n",
    "        model.fit(X_train, y_train, pattern='newton', lr=1)\n",
    "        result = model.predict(X_test)\n",
    "        delta = result-y_test\n",
    "        num = 0\n",
    "        for i in range(delta.shape[0]):\n",
    "            if delta[i] == 0:\n",
    "                num += 1\n",
    "        acc.append(num / delta.shape[0])\n",
    "print(sum(acc)/len(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8137254901960789\n"
     ]
    }
   ],
   "source": [
    "# 牛顿法(保留第69列特征)\n",
    "from Logistic import LogisticRegression\n",
    "np.random.seed(37)\n",
    "df = df.apply(lambda x: (x-np.min(x))/(np.max(x)-np.min(x)))\n",
    "_df = df.iloc[:,[6,9,11]]\n",
    "positive = np.array(_df.iloc[(_df.iloc[:, 2] == 1.0).values, :])\n",
    "negative = np.array(_df.iloc[(~(_df.iloc[:, 2] == 1.0)).values, :])\n",
    "np.random.shuffle(positive)\n",
    "np.random.shuffle(negative)\n",
    "pn = positive.shape[0]\n",
    "nn = negative.shape[0]\n",
    "acc = []\n",
    "model = LogisticRegression(gamma=0, plot=False)\n",
    "for i in range(10):\n",
    "    for i in range(4):\n",
    "        test_index_p = set(range(i*int(pn/4), (i+1)*int(pn/4)))\n",
    "        train_index_p = set(range(pn)) - test_index_p\n",
    "        (train_p, test_p) = (positive[np.array(list(train_index_p)), :],\n",
    "                             positive[np.array(list(test_index_p)), :])\n",
    "        test_index_n = set(range(i*int(nn/4), (i+1)*int(nn/4)))\n",
    "        train_index_n = set(range(nn)) - test_index_n\n",
    "        (train_n, test_n) = (negative[np.array(list(train_index_n)), :],\n",
    "                             negative[np.array(list(test_index_n)), :])\n",
    "        (train, test) = (np.r_[train_p, train_n], np.r_[test_p, test_n])\n",
    "        (X_train, y_train) = (train[:, :-1], train[:, -1:])\n",
    "        (X_test, y_test) = (test[:, :-1], test[:, -1:])\n",
    "        X_train = np.append(X_train, np.ones((X_train.shape[0], 1)), axis=1)\n",
    "        X_test = np.append(X_test, np.ones((X_test.shape[0], 1)), axis=1)\n",
    "        model.fit(X_train, y_train, pattern='newton', lr=1)\n",
    "        result = model.predict(X_test)\n",
    "        delta = result-y_test\n",
    "        num = 0\n",
    "        for i in range(delta.shape[0]):\n",
    "            if delta[i] == 0:\n",
    "                num += 1\n",
    "        acc.append(num / delta.shape[0])\n",
    "print(sum(acc)/len(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8120915032679743\n"
     ]
    }
   ],
   "source": [
    "# 牛顿法(保留第9列特征)\n",
    "from Logistic import LogisticRegression\n",
    "np.random.seed(37)\n",
    "df = df.apply(lambda x: (x-np.min(x))/(np.max(x)-np.min(x)))\n",
    "_df = df.iloc[:,[9,11]]\n",
    "positive = np.array(_df.iloc[(_df.iloc[:, 1] == 1.0).values, :])\n",
    "negative = np.array(_df.iloc[(~(_df.iloc[:, 1] == 1.0)).values, :])\n",
    "np.random.shuffle(positive)\n",
    "np.random.shuffle(negative)\n",
    "pn = positive.shape[0]\n",
    "nn = negative.shape[0]\n",
    "acc = []\n",
    "model = LogisticRegression(gamma=0, plot=False)\n",
    "for i in range(10):\n",
    "    for i in range(4):\n",
    "        test_index_p = set(range(i*int(pn/4), (i+1)*int(pn/4)))\n",
    "        train_index_p = set(range(pn)) - test_index_p\n",
    "        (train_p, test_p) = (positive[np.array(list(train_index_p)), :],\n",
    "                             positive[np.array(list(test_index_p)), :])\n",
    "        test_index_n = set(range(i*int(nn/4), (i+1)*int(nn/4)))\n",
    "        train_index_n = set(range(nn)) - test_index_n\n",
    "        (train_n, test_n) = (negative[np.array(list(train_index_n)), :],\n",
    "                             negative[np.array(list(test_index_n)), :])\n",
    "        (train, test) = (np.r_[train_p, train_n], np.r_[test_p, test_n])\n",
    "        (X_train, y_train) = (train[:, :-1], train[:, -1:])\n",
    "        (X_test, y_test) = (test[:, :-1], test[:, -1:])\n",
    "        X_train = np.append(X_train, np.ones((X_train.shape[0], 1)), axis=1)\n",
    "        X_test = np.append(X_test, np.ones((X_test.shape[0], 1)), axis=1)\n",
    "        model.fit(X_train, y_train, pattern='newton', lr=1)\n",
    "        result = model.predict(X_test)\n",
    "        delta = result-y_test\n",
    "        num = 0\n",
    "        for i in range(delta.shape[0]):\n",
    "            if delta[i] == 0:\n",
    "                num += 1\n",
    "        acc.append(num / delta.shape[0])\n",
    "print(sum(acc)/len(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c58e9361bde7ca617934da376e83056db506761bdc9593ca2087fabac973f609"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
